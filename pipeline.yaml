# PIPELINE DEFINITION
# Name: my-pipeline
# Description: My ML pipeline.
components:
  comp-createpvc:
    executorLabel: exec-createpvc
    inputDefinitions:
      parameters:
        access_modes:
          description: 'AccessModes to request for the provisioned PVC. May

            be one or more of ``''ReadWriteOnce''``, ``''ReadOnlyMany''``, ``''ReadWriteMany''``,
            or

            ``''ReadWriteOncePod''``. Corresponds to `PersistentVolumeClaim.spec.accessModes
            <https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes>`_.'
          parameterType: LIST
        annotations:
          description: Annotations for the PVC's metadata. Corresponds to `PersistentVolumeClaim.metadata.annotations
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaim>`_.
          isOptional: true
          parameterType: STRUCT
        pvc_name:
          description: 'Name of the PVC. Corresponds to `PersistentVolumeClaim.metadata.name
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaim>`_.
            Only one of ``pvc_name`` and ``pvc_name_suffix`` can

            be provided.'
          isOptional: true
          parameterType: STRING
        pvc_name_suffix:
          description: 'Prefix to use for a dynamically generated name, which

            will take the form ``<argo-workflow-name>-<pvc_name_suffix>``. Only one

            of ``pvc_name`` and ``pvc_name_suffix`` can be provided.'
          isOptional: true
          parameterType: STRING
        size:
          description: The size of storage requested by the PVC that will be provisioned.
            For example, ``'5Gi'``. Corresponds to `PersistentVolumeClaim.spec.resources.requests.storage
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaimSpec>`_.
          parameterType: STRING
        storage_class_name:
          defaultValue: ''
          description: 'Name of StorageClass from which to provision the PV

            to back the PVC. ``None`` indicates to use the cluster''s default

            storage_class_name. Set to ``''''`` for a statically specified PVC.'
          isOptional: true
          parameterType: STRING
        volume_name:
          description: 'Pre-existing PersistentVolume that should back the

            provisioned PersistentVolumeClaim. Used for statically

            specified PV only. Corresponds to `PersistentVolumeClaim.spec.volumeName
            <https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/#PersistentVolumeClaimSpec>`_.'
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        name:
          parameterType: STRING
  comp-prepare-data:
    executorLabel: exec-prepare-data
    outputDefinitions:
      artifacts:
        output_csv:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-train-test-split:
    executorLabel: exec-train-test-split
    inputDefinitions:
      artifacts:
        input_csv:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        output_dir:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-training-basic-classifier:
    executorLabel: exec-training-basic-classifier
    inputDefinitions:
      artifacts:
        input_dir:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        model_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-createpvc:
      container:
        image: argostub/createpvc
    exec-prepare-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - prepare_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy'\
          \ 'scikit-learn' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef prepare_data(output_csv: dsl.Output[dsl.Artifact]):\n    import\
          \ pandas as pd\n    import os\n    from sklearn import datasets\n\n    #\
          \ Load dataset\n    iris = datasets.load_iris()\n    df = pd.DataFrame(iris.data,\
          \ columns=iris.feature_names)\n    df['species'] = iris.target\n\n    #\
          \ Save the prepared data to CSV\n    df = df.dropna()\n\n    # Ensure the\
          \ output directory exists\n    os.makedirs(output_csv.path, exist_ok=True)\n\
          \n    output_csv_path = os.path.join(output_csv.path, 'final_df.csv')\n\
          \    df.to_csv(output_csv_path, index=False)\n\n"
        image: python:3.9
    exec-train-test-split:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_test_split
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy'\
          \ 'scikit-learn' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_test_split(input_csv: dsl.Input[dsl.Artifact], output_dir:\
          \ dsl.Output[dsl.Artifact]):\n    import pandas as pd\n    import numpy\
          \ as np\n    import os\n    from sklearn.model_selection import train_test_split\n\
          \n    # Load the prepared data\n    final_data = pd.read_csv(os.path.join(input_csv.path,\
          \ 'final_df.csv'))\n\n    # Split the data into training and testing sets\n\
          \    target_column = 'species'\n    X = final_data.loc[:, final_data.columns\
          \ != target_column]\n    y = final_data.loc[:, final_data.columns == target_column]\n\
          \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\
          \ stratify=y, random_state=47)\n\n    # Ensure the output directory exists\n\
          \    os.makedirs(output_dir.path, exist_ok=True)\n\n    # Save the splits\
          \ to .npy files\n    np.save(os.path.join(output_dir.path, 'X_train.npy'),\
          \ X_train)\n    np.save(os.path.join(output_dir.path, 'X_test.npy'), X_test)\n\
          \    np.save(os.path.join(output_dir.path, 'y_train.npy'), y_train)\n  \
          \  np.save(os.path.join(output_dir.path, 'y_test.npy'), y_test)\n\n"
        image: python:3.9
    exec-training-basic-classifier:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - training_basic_classifier
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy'\
          \ 'scikit-learn' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef training_basic_classifier(input_dir: dsl.Input[dsl.Artifact],\
          \ model_output: dsl.Output[dsl.Artifact]):\n    import numpy as np\n   \
          \ import os\n    from sklearn.linear_model import LogisticRegression\n \
          \   import pickle\n\n    # Load the training data\n    X_train = np.load(os.path.join(input_dir.path,\
          \ 'X_train.npy'), allow_pickle=True)\n    y_train = np.load(os.path.join(input_dir.path,\
          \ 'y_train.npy'), allow_pickle=True)\n\n    # Train the logistic regression\
          \ classifier\n    classifier = LogisticRegression(max_iter=500)\n    classifier.fit(X_train,\
          \ y_train)\n\n    # Ensure the output directory exists\n    os.makedirs(model_output.path,\
          \ exist_ok=True)\n    print(model_output.path)\n    # Save the trained model\
          \ to a pickle file\n    model_path = os.path.join(model_output.path, 'model.pkl')\n\
          \    with open('/trained_model/model.pkl', 'wb') as f:\n        pickle.dump(classifier,\
          \ f)\n\n"
        image: python:3.9
pipelineInfo:
  description: My ML pipeline.
  name: my-pipeline
root:
  dag:
    tasks:
      createpvc:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-createpvc
        inputs:
          parameters:
            access_modes:
              runtimeValue:
                constant:
                - ReadWriteOnce
            pvc_name:
              runtimeValue:
                constant: trained-model-v3
            size:
              runtimeValue:
                constant: 5Gi
            storage_class_name:
              runtimeValue:
                constant: local-path
        taskInfo:
          name: createpvc
      prepare-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-prepare-data
        taskInfo:
          name: prepare-data
      train-test-split:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-test-split
        dependentTasks:
        - prepare-data
        inputs:
          artifacts:
            input_csv:
              taskOutputArtifact:
                outputArtifactKey: output_csv
                producerTask: prepare-data
        taskInfo:
          name: train-test-split
      training-basic-classifier:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-training-basic-classifier
        dependentTasks:
        - createpvc
        - train-test-split
        inputs:
          artifacts:
            input_dir:
              taskOutputArtifact:
                outputArtifactKey: output_dir
                producerTask: train-test-split
        taskInfo:
          name: training-basic-classifier
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-training-basic-classifier:
          pvcMount:
          - mountPath: /trained_model
            taskOutputParameter:
              outputParameterKey: name
              producerTask: createpvc
