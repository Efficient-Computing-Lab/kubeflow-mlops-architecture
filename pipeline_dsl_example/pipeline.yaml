# PIPELINE DEFINITION
# Name: epos-pipeline
# Description: My ML pipeline.
components:
  comp-configure-parameters:
    executorLabel: exec-configure-parameters
  comp-train:
    executorLabel: exec-train
deploymentSpec:
  executors:
    exec-configure-parameters:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - configure_parameters
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef configure_parameters():\n    import subprocess\n    import shutil\n\
          \    import os\n    import yaml\n    command=\"rm -r /datasets/params.yml\"\
          \n    subprocess.run(command, shell=True)\n    new_params_yaml ={\n    \
          \  \"dataset\": \"carObj12\",\n      \"model_variant\": \"xception_65\"\
          ,\n      \"atrous_rates\": [12, 24, 36],\n      \"encoder_output_stride\"\
          : 8,\n      \"decoder_output_stride\": [4],\n      \"upsample_logits\":\
          \ False,\n      \"frag_seg_agnostic\": False,\n      \"frag_loc_agnostic\"\
          : False,\n      \"num_frags\": 64,\n      \"corr_min_obj_conf\": 0.1,\n\
          \      \"corr_min_frag_rel_conf\": 0.5,\n      \"corr_project_to_model\"\
          : False,\n      \"train_tfrecord_names\": [\"carObj12_train-primesense\"\
          ],\n      \"train_max_height_before_crop\": 480,\n      \"train_crop_size\"\
          : \"640, 480\",\n      \"freeze_regex_list\": [\"xception_65/entry_flow\"\
          ],\n      \"initialize_last_layer\": True,\n      \"fine_tune_batch_norm\"\
          : False,\n      \"train_steps\": 15480,\n      \"train_batch_size\": 1,\n\
          \      \"base_learning_rate\": 0.0000001,\n      \"learning_power\": 0.9,\n\
          \      \"obj_cls_loss_weight\": 1.0,\n      \"frag_cls_loss_weight\": 1.0,\n\
          \      \"frag_loc_loss_weight\": 100.0,\n      \"train_knn_frags\": 1,\n\
          \      \"data_augmentations\": {\n        \"random_adjust_brightness\":\
          \ {\n          \"min_delta\": -0.2,\n          \"max_delta\": 0.4\n    \
          \    },\n        \"random_adjust_contrast\": {\n          \"min_delta\"\
          : 0.6,\n          \"max_delta\": 1.4\n        },\n        \"random_adjust_saturation\"\
          : {\n          \"min_delta\": 0.6,\n          \"max_delta\": 1.4\n     \
          \   },\n        \"random_adjust_hue\": {\n          \"max_delta\": 1.0\n\
          \        },\n        \"random_blur\": {\n          \"max_sigma\": 1.5\n\
          \        },\n        \"random_gaussian_noise\": {\n          \"max_sigma\"\
          : 0.1\n        }\n      },\n      \"eval_tfrecord_names\": [\"tless_test_targets-bop19\"\
          ],\n      \"eval_max_height_before_crop\": 480,\n      \"eval_crop_size\"\
          : \"640, 480\",\n      \"infer_tfrecord_names\": [\"carObj12_test\"],\n\
          \      \"infer_max_height_before_crop\": 480,\n      \"infer_crop_size\"\
          : \"640, 480\"\n    }\n    # Save the dictionary to a YAML file\n    with\
          \ open('/app/store/tf_models/obj12/params.yml', 'w') as file:\n        yaml.dump(new_params_yaml,\
          \ file, default_flow_style=False)\n    # Define source and destination paths\n\
          \    training_set_source = \"/datasets/epos/training_set\"\n    training_set_destination\
          \ = \"/app/datasets/carObj12/train_primesense\"\n\n    # Create the destination\
          \ directory if it doesn't exist\n    os.makedirs(os.path.dirname(training_set_destination),\
          \ exist_ok=True)\n\n    # Move the directory\n    shutil.copytree(training_set_source,\
          \ training_set_destination)\n\n"
        image: gkorod/topo:1.0
    exec-train:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train():\n    import subprocess\n    import shutil\n    import\
          \ os\n    command = \"source /root/.bashrc && conda activate epos && exec\
          \ bash\"\n    subprocess.run(\"conda list\", shell=True)\n    # Run the\
          \ command in a new shell\n    subprocess.run(command, shell=True, executable='/bin/bash')\n\
          \    subprocess.run([\n        'python', 'epos/scripts/create_example_list.py',\n\
          \        '--dataset=carObj12',\n        '--split=train',\n        '--split_type=primesense'\n\
          \    ])\n\n    # Step 3: Run the Python script to create the TFRecord\n\
          \    subprocess.run([\n        'python', 'epos/scripts/create_tfrecord.py',\n\
          \        '--dataset=carObj12',\n        '--split=train',\n        '--split_type=primesense',\n\
          \        '--examples_filename=carObj12_train-primesense_examples.txt',\n\
          \        '--add_gt=True',\n        '--shuffle=True',\n        '--rgb_format=png'\n\
          \    ], shell=True)\n\n    # Step 4: Create the output directory\n    os.makedirs('/app/store/tf_models/obj12',\
          \ exist_ok=True)\n\n    # Step 5: Run the Python script to train the model\n\
          \    subprocess.run(['python', 'epos/scripts/train.py', '--model=obj12'],shell=True)\n\
          \    contents = os.listdir(\"/app/store/tf_models/obj12\")\n\n    # Source\
          \ file path\n    src = '/app/store/tf_models'\n\n    # Destination file\
          \ path\n    dst = '/trained_models'\n\n    # Copy the file\n    shutil.copytree(src,\
          \ dst,dirs_exist_ok=True)\n\n"
        image: gkorod/topo:1.0
        resources:
          accelerator:
            count: '1'
            type: nvidia.com/gpu
pipelineInfo:
  description: My ML pipeline.
  name: epos-pipeline
root:
  dag:
    tasks:
      configure-parameters:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-configure-parameters
        taskInfo:
          name: configure-parameters
      train:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train
        taskInfo:
          name: train
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-configure-parameters:
          pvcMount:
          - constant: datasets
            mountPath: /datasets
        exec-train:
          podMetadata:
            annotations:
              runtimeClassName: nvidia
          pvcMount:
          - constant: trained-models
            mountPath: /trained_models
          - constant: datasets
            mountPath: /datasets
